{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3933932169061679741\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4937233203\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16991730595437065990\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "(5923, 28, 28, 1)\n",
      "(60000,)\n",
      "(980, 28, 28, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# double check your tf version\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# get sampling data set\n",
    "fashion_mnist = keras.datasets.mnist\n",
    "(trainx, trainy), (testx, testy) = fashion_mnist.load_data()\n",
    "trainx = (trainx.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "trainx = trainx[trainy == 0]\n",
    "testx = testx[testy == 0]\n",
    "\n",
    "trainx = trainx.reshape(trainx.shape[0],28,28,1)\n",
    "testx = testx.reshape(testx.shape[0],28,28,1)\n",
    "\n",
    "print(trainx.shape)\n",
    "print(trainy.shape)\n",
    "print(testx.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gluon.mxnet.io/_images/dcgan.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://gluon.mxnet.io/_images/dcgan.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGAN():\n",
    "    def __init__(self, width=28, height=28, channels=1):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channels = channels\n",
    "        self.shape = (self.width, self.height, self.channels)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5, decay=8e-8)\n",
    "       \n",
    "        self.D = self.__discriminator()\n",
    "        self.D.compile(loss='binary_crossentropy', optimizer=self.optimizer, metrics=['accuracy'])\n",
    "\n",
    "        self.G = self.__generator()\n",
    "        self.G.compile(loss='binary_crossentropy', optimizer=self.optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        self.GAN = self.__gan()\n",
    "        self.GAN.compile(loss='binary_crossentropy', optimizer=self.optimizer)        \n",
    "      \n",
    "    # use vector as input.\n",
    "    def __generator(self):\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                # GAN generator need to generate a batch of Samples, so here is 256 images with small size 7*7\n",
    "                # flatten layer: 7*7\n",
    "                tf.keras.layers.Dense(256*7*7, input_shape=(100,)),                \n",
    "                tf.keras.layers.BatchNormalization(),  # 使用 BatchNormalization 優化\n",
    "                tf.keras.layers.LeakyReLU(),   # 使用 LeakyReLU 激活函數\n",
    "                \n",
    "                # reshape\n",
    "                tf.keras.layers.Reshape((7, 7, 256)),\n",
    "                ### model.output_shape == (None, 7,7,256)\n",
    "                \n",
    "                # layer 2\n",
    "                # self.G.add(UpSampling2D())\n",
    "                tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
    "                 ### model.output_shape == (None, 7,7,128)\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.LeakyReLU(),\n",
    "                                \n",
    "                # layer 3\n",
    "                # self.G.add(UpSampling2D())\n",
    "                tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "                 ### model.output_shape == (None, 14,14,64)\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.LeakyReLU(),\n",
    "                \n",
    "                # layer 4\n",
    "                tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\n",
    "                ### model.output_shape == (None, 28,28,1)        \n",
    "            ]\n",
    "        )\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def __discriminator(self):\n",
    "        model = tf.keras.Sequential( \n",
    "            [\n",
    "                # input 28*28, 64 filters and each is 5 * 5 , stride means step size for filter              \n",
    "                tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',input_shape=[28, 28, 1]),\n",
    "                tf.keras.layers.LeakyReLU(),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                \n",
    "                # input 14*14\n",
    "                tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "                tf.keras.layers.LeakyReLU(),\n",
    "                tf.keras.layers.Dropout(0.3),                \n",
    "                \n",
    "                tf.keras.layers.Conv2D(64*4, (5, 5), strides=(2, 2), padding='same'),\n",
    "                tf.keras.layers.LeakyReLU(),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                \n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ]\n",
    "        )\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def __gan(self):\n",
    "        self.D.trainable = False\n",
    "        self.G.trainable = True\n",
    "\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                self.G,\n",
    "                self.D\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def __generator_loss(fake_output):\n",
    "        cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        \n",
    "        return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    \n",
    "    def __discriminator_loss(real_output, fake_output):\n",
    "        # This method returns a helper function to compute cross entropy loss\n",
    "        cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        \n",
    "        real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        total_loss = real_loss + fake_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    # 1. fix Generator's parameter and generate randomly sample vector output.\n",
    "    # 2. get real sample from DB\n",
    "    # 3. Train Discriminator with real sample + bad sample\n",
    "    # 4. fix Discriminator's parameter and train Generator.\n",
    "\n",
    "    def train_test(self, X_train, epochs=50, batch = 32, save_interval = 100):\n",
    "        for epoch in range(epochs):\n",
    "            index = np.random.randint(0, X_train.shape[0], size=batch)\n",
    "            images_train = X_train[index]\n",
    "            \n",
    "            noise = np.random.normal(0.0,1.0,(batch, 100))\n",
    "            images_fake = self.G(noise)\n",
    "            \n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.concatenate((np.ones((batch, 1)), np.zeros((batch, 1))))\n",
    "            \n",
    "            d_loss = self.D.train_on_batch(x,y)\n",
    "\n",
    "            #y[batch:,:] = 1\n",
    "            noise = np.random.normal(0, 1.0, size=[2*batch, 100])\n",
    "            y_mislabeld = np.ones((2*batch, 1))\n",
    "            \n",
    "            a_loss = self.GAN.train_on_batch(noise, y_mislabeld)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print('epoch: ',epoch,', [Discriminator :: d_loss: ',d_loss,'], [ Generator :: loss: ',a_loss,']')           \n",
    "                self.plot_images(True,16,epoch)\n",
    "         \n",
    "    def plot_images(self, save2file=False, samples=16, step=0):\n",
    "        ''' Plot and generated images '''\n",
    "        filename = \"./Result/tf20_dcgan/mnist_%d.png\" % step\n",
    "        noise = np.random.normal(0, 1, (samples, 100))\n",
    "\n",
    "        images = self.G.predict(noise)\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.height, self.width]).astype('float32')\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 1,030,145\n",
      "Trainable params: 1,030,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 7, 7, 128)         819200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 64)        204800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         1600      \n",
      "=================================================================\n",
      "Total params: 2,343,488\n",
      "Trainable params: 2,318,016\n",
      "Non-trainable params: 25,472\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1020 17:03:39.566539  8796 base_layer.py:1814] Layer dense_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5923\n",
      "epoch:  0 , [Discriminator :: d_loss:  [3.1855884, 0.5] ], [ Generator :: loss:  3.099935 ]\n",
      "epoch:  100 , [Discriminator :: d_loss:  [0.0, 1.0] ], [ Generator :: loss:  0.0 ]\n",
      "epoch:  200 , [Discriminator :: d_loss:  [0.0018986292, 1.0] ], [ Generator :: loss:  0.0 ]\n",
      "epoch:  300 , [Discriminator :: d_loss:  [0.02529014, 0.984375] ], [ Generator :: loss:  0.00085635914 ]\n",
      "epoch:  400 , [Discriminator :: d_loss:  [7.7124743, 0.5] ], [ Generator :: loss:  15.424949 ]\n",
      "epoch:  500 , [Discriminator :: d_loss:  [0.0058436114, 1.0] ], [ Generator :: loss:  0.1024688 ]\n",
      "epoch:  600 , [Discriminator :: d_loss:  [0.19720873, 0.921875] ], [ Generator :: loss:  4.969721 ]\n",
      "epoch:  700 , [Discriminator :: d_loss:  [1.5223641, 0.8125] ], [ Generator :: loss:  5.142622 ]\n",
      "epoch:  800 , [Discriminator :: d_loss:  [0.40199584, 0.84375] ], [ Generator :: loss:  3.205265 ]\n",
      "epoch:  900 , [Discriminator :: d_loss:  [0.2425319, 0.96875] ], [ Generator :: loss:  3.7721841 ]\n",
      "epoch:  1000 , [Discriminator :: d_loss:  [0.29880977, 0.84375] ], [ Generator :: loss:  3.1743417 ]\n",
      "epoch:  1100 , [Discriminator :: d_loss:  [0.5513412, 0.90625] ], [ Generator :: loss:  3.4333498 ]\n",
      "epoch:  1200 , [Discriminator :: d_loss:  [0.19510701, 0.921875] ], [ Generator :: loss:  5.8900156 ]\n",
      "epoch:  1300 , [Discriminator :: d_loss:  [6.008892, 0.515625] ], [ Generator :: loss:  10.594385 ]\n",
      "epoch:  1400 , [Discriminator :: d_loss:  [0.5004371, 0.734375] ], [ Generator :: loss:  2.4076066 ]\n",
      "epoch:  1500 , [Discriminator :: d_loss:  [0.5373261, 0.890625] ], [ Generator :: loss:  3.1009688 ]\n",
      "epoch:  1600 , [Discriminator :: d_loss:  [0.13625726, 0.921875] ], [ Generator :: loss:  9.581784 ]\n",
      "epoch:  1700 , [Discriminator :: d_loss:  [0.111948125, 0.9375] ], [ Generator :: loss:  10.642531 ]\n",
      "epoch:  1800 , [Discriminator :: d_loss:  [0.20534712, 0.890625] ], [ Generator :: loss:  9.795786 ]\n",
      "epoch:  1900 , [Discriminator :: d_loss:  [0.08153692, 0.953125] ], [ Generator :: loss:  10.784085 ]\n",
      "epoch:  2000 , [Discriminator :: d_loss:  [0.1067092, 0.9375] ], [ Generator :: loss:  11.345857 ]\n",
      "epoch:  2100 , [Discriminator :: d_loss:  [0.0709026, 0.984375] ], [ Generator :: loss:  11.385937 ]\n",
      "epoch:  2200 , [Discriminator :: d_loss:  [0.053424347, 0.984375] ], [ Generator :: loss:  12.09593 ]\n",
      "epoch:  2300 , [Discriminator :: d_loss:  [0.076307476, 0.984375] ], [ Generator :: loss:  10.319038 ]\n",
      "epoch:  2400 , [Discriminator :: d_loss:  [0.08501138, 0.96875] ], [ Generator :: loss:  10.383707 ]\n",
      "epoch:  2500 , [Discriminator :: d_loss:  [0.117328696, 0.96875] ], [ Generator :: loss:  10.503787 ]\n",
      "epoch:  2600 , [Discriminator :: d_loss:  [0.06459338, 0.984375] ], [ Generator :: loss:  10.594247 ]\n",
      "epoch:  2700 , [Discriminator :: d_loss:  [0.06910779, 0.984375] ], [ Generator :: loss:  12.656831 ]\n",
      "epoch:  2800 , [Discriminator :: d_loss:  [0.13434349, 0.9375] ], [ Generator :: loss:  12.179955 ]\n",
      "epoch:  2900 , [Discriminator :: d_loss:  [7.7124743, 0.5] ], [ Generator :: loss:  15.424949 ]\n",
      "epoch:  3000 , [Discriminator :: d_loss:  [7.7124743, 0.5] ], [ Generator :: loss:  15.424949 ]\n",
      "epoch:  3100 , [Discriminator :: d_loss:  [7.472088, 0.515625] ], [ Generator :: loss:  15.424949 ]\n",
      "epoch:  3200 , [Discriminator :: d_loss:  [7.0665703, 0.515625] ], [ Generator :: loss:  14.949473 ]\n",
      "epoch:  3300 , [Discriminator :: d_loss:  [1.9901298, 0.75] ], [ Generator :: loss:  6.3714657 ]\n",
      "epoch:  3400 , [Discriminator :: d_loss:  [0.43216065, 0.921875] ], [ Generator :: loss:  6.747358 ]\n",
      "epoch:  3500 , [Discriminator :: d_loss:  [0.5176504, 0.828125] ], [ Generator :: loss:  5.2863383 ]\n",
      "epoch:  3600 , [Discriminator :: d_loss:  [0.25969666, 0.890625] ], [ Generator :: loss:  6.6589375 ]\n",
      "epoch:  3700 , [Discriminator :: d_loss:  [0.2796675, 0.890625] ], [ Generator :: loss:  7.6193104 ]\n",
      "epoch:  3800 , [Discriminator :: d_loss:  [0.28407288, 0.859375] ], [ Generator :: loss:  10.723579 ]\n",
      "epoch:  3900 , [Discriminator :: d_loss:  [0.37595576, 0.90625] ], [ Generator :: loss:  10.451864 ]\n",
      "epoch:  4000 , [Discriminator :: d_loss:  [0.10375106, 0.953125] ], [ Generator :: loss:  10.397062 ]\n",
      "epoch:  4100 , [Discriminator :: d_loss:  [0.13492957, 0.96875] ], [ Generator :: loss:  9.857217 ]\n",
      "epoch:  4200 , [Discriminator :: d_loss:  [0.41709152, 0.90625] ], [ Generator :: loss:  10.15645 ]\n",
      "epoch:  4300 , [Discriminator :: d_loss:  [0.1261582, 0.984375] ], [ Generator :: loss:  11.822557 ]\n",
      "epoch:  4400 , [Discriminator :: d_loss:  [0.18530431, 0.921875] ], [ Generator :: loss:  7.761196 ]\n",
      "epoch:  4500 , [Discriminator :: d_loss:  [0.4207183, 0.9375] ], [ Generator :: loss:  7.8049154 ]\n",
      "epoch:  4600 , [Discriminator :: d_loss:  [0.18337086, 0.9375] ], [ Generator :: loss:  8.448352 ]\n",
      "epoch:  4700 , [Discriminator :: d_loss:  [0.11930914, 0.96875] ], [ Generator :: loss:  7.7694364 ]\n",
      "epoch:  4800 , [Discriminator :: d_loss:  [0.2448055, 0.90625] ], [ Generator :: loss:  9.220879 ]\n",
      "epoch:  4900 , [Discriminator :: d_loss:  [0.21062283, 0.90625] ], [ Generator :: loss:  7.3384953 ]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.set_floatx('float32')\n",
    "#gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "\n",
    "go = myGAN()\n",
    "print(len(trainx))\n",
    "go.train_test(trainx, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
